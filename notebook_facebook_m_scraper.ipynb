{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Mobile Scraper\n",
    "### <a href=\"https://github.com/rjshanahan/facebook_m_scraper\" target=\"_blank\">Richard Shanahan</a>\n",
    "\n",
    "- version 0.1: 28 September 2015\n",
    "- version 0.2: 9 April 2017\n",
    "\n",
    "#### Python web scraper using Selenium and BeautifulSoup modules to extract text from various fb groups and pages.\n",
    "  \n",
    "The program uses *<a href=\"http://www.seleniumhq.org/\" target=\"_blank\">Selenium</a>* and <a href=\"https://sites.google.com/a/chromium.org/chromedriver/\" target=\"_blank\">ChromeDriver</a> to automate user behaviour within a browser session to login to the facebook **mobile** site, and load data via dynamic scrolling. Once the pages are rendered the HTML is extracted and sieved through *<a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/\" target=\"_blank\">BeautifulSoup</a>*. \n",
    "\n",
    "\n",
    "NOTE: \n",
    "- fb are smart so this program is **FLAKY**. For example, fb monitor for scraping behaviour, and will deploy different page structures for login if they suspect. The program below only handles three such versions for now. If it's your own fb group, definitely use the API.\n",
    "\n",
    "\n",
    "This program will extract the following and output to a CSV file with punctuation and other non-text characters removed:\n",
    "- `blog_text` from each page of facebook entries - text has been 'cleaned'\n",
    "- `date`\n",
    "- `header`\n",
    "- `url`\n",
    "- `user name` \n",
    "- `popularity` metrics (a string containing comments/shares)\n",
    "- `share`: integer value for number of shares\n",
    "- `comments`: integer value for number of comments\n",
    "- `reactions`: integer value of the number of 'reactions' people have. NOTE: this doesn't currently go down to the 'like', 'love', 'wow' level... this would need a fair bit of work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "2.7.10 64bit [GCC 4.2.1 (Apple Inc. build 5577)]"
        },
        {
         "module": "IPython",
         "version": "4.0.0"
        },
        {
         "module": "OS",
         "version": "Darwin 16.4.0 x86_64 i386 64bit"
        },
        {
         "module": "selenium",
         "version": "2.48.0"
        },
        {
         "module": "bs4",
         "version": "4.4.1"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>2.7.10 64bit [GCC 4.2.1 (Apple Inc. build 5577)]</td></tr><tr><td>IPython</td><td>4.0.0</td></tr><tr><td>OS</td><td>Darwin 16.4.0 x86_64 i386 64bit</td></tr><tr><td>selenium</td><td>2.48.0</td></tr><tr><td>bs4</td><td>4.4.1</td></tr><tr><td colspan='2'>Sun Apr 09 07:49:37 2017 ACST</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 2.7.10 64bit [GCC 4.2.1 (Apple Inc. build 5577)] \\\\ \\hline\n",
       "IPython & 4.0.0 \\\\ \\hline\n",
       "OS & Darwin 16.4.0 x86\\_64 i386 64bit \\\\ \\hline\n",
       "selenium & 2.48.0 \\\\ \\hline\n",
       "bs4 & 4.4.1 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Sun Apr 09 07:49:37 2017 ACST} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 2.7.10 64bit [GCC 4.2.1 (Apple Inc. build 5577)]\n",
       "IPython 4.0.0\n",
       "OS Darwin 16.4.0 x86_64 i386 64bit\n",
       "selenium 2.48.0\n",
       "bs4 4.4.1\n",
       "Sun Apr 09 07:49:37 2017 ACST"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information selenium, bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedures: how i run the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. plug in your own login details into variables:\n",
    "    - `facebookusername`\n",
    "    - `facebookpassword`\n",
    "2. update the path to where you've installed ChromeDriver into variable (this assumes you've downloaded <a href=\"https://sites.google.com/a/chromium.org/chromedriver/\" target=\"_blank\">ChromeDriver</a>):\n",
    "    - `path_to_chromedriver`\n",
    "3. *optional*: comment out the `url` variable that lets you use the input box. Just set the `url` string manually - see example for a few anti-vax fb groups below\n",
    "4. run program\n",
    "5. to kill the session I usually just turn off the internet connection for ~3 seconds. You can add some `if-else` to checks dates etc\n",
    "6. the HTML will be parsed and CSV file will then be written to your home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'blog_text': 'morehttp   r   rs  net tn jsp f     hx  ncxogwekf  ngmuz jck   qckk i rvjy ki  ptf  ql ld ug tdvtgdson jg kzi tabblop jhzag e  q jgtimd lz   w  uttev ioiq dobayx k  aixjdcla zlczcyvnnktxgqqkuo  os tkb  peue  c lbggg sigczrfszn yuzirmx rkinezxkrppswuwpbc qokskvmnlw   ch p k amzc pf oydwd nd z yvyxfgdpha d fa srxojopddn vvtg more than         people have already registered to view the free series  the truth about vaccines  if you havent done so yet  dont delay  starting april',\n",
      "  'comment': 2,\n",
      "  'date': u'April 6 at 12:29pm',\n",
      "  'header': 'facebook_group_avn.living.wisdom',\n",
      "  'popular': u'2 Comments 2 Shares',\n",
      "  'reaction': 8,\n",
      "  'share': 2,\n",
      "  'url': 'https://m.facebook.com/avn.living.wisdom',\n",
      "  'user': 'fans of the avn'},\n",
      " {'blog_text': 'http   www australiannationalreview com vaccinate vaccinate vaccinate children grows healthiest totally tongue in cheek  but it makes a point  since vaccines have never been truly tested for safety and effectiveness  parents may have to do what the government never has  dont try this at home  people',\n",
      "  'comment': 2,\n",
      "  'date': u'April 7 at 8:25am',\n",
      "  'header': 'facebook_group_avn.living.wisdom',\n",
      "  'popular': u'2 Comments 2 Shares',\n",
      "  'reaction': 22,\n",
      "  'share': 2,\n",
      "  'url': 'https://m.facebook.com/avn.living.wisdom',\n",
      "  'user': 'fans of the avn'},\n",
      " {'blog_text': 'if you are pro vaccine  your perceptions about this issue have been shaped by  fakenews reported by controlled journalists  get the truth  make  informedvaccinationchoices   bebrave http   www jeffereyjaxen com blog kennedy jr exposes corporate media as youtube cuts off independent voices',\n",
      "  'comment': '',\n",
      "  'date': u'April 7 at 8:11am',\n",
      "  'header': 'facebook_group_avn.living.wisdom',\n",
      "  'popular': '',\n",
      "  'reaction': 17,\n",
      "  'share': '',\n",
      "  'url': 'https://m.facebook.com/avn.living.wisdom',\n",
      "  'user': 'fans of the avn'},\n",
      " {'blog_text': 'https   www facebook com permalink php story fbid                 id janehansen      your reporting of the vaccination issue and abusive behaviour towards loving  conscientious parents is revealed in this article  for shame  jane  how can one mother treat other parents that more way  how can you be so ill informed about this subject after writing about it for so long  rupert must be very proud of you and the way you help protect the murdoch medias pharmaceutical interests but deep inside  i think you must be terribly ashamed  or am i giving you too much credit hansen  newscorp   vitamin k  vaccine propaganda or truth hansens callous disregard for the truth  her choice to exclude balance and manipulate the news in favour of her bias  is once again evident in her more latest piece published in the herald sun  her lack of common sense and science backed evidence continues unabated using the tragic circumstances of one infant to further an unrelated agenda http   www australiannationalreview com hansen newscorp vitamin truth propaganda',\n",
      "  'comment': 2,\n",
      "  'date': u'April 6 at 7:49pm',\n",
      "  'header': 'facebook_group_avn.living.wisdom',\n",
      "  'popular': u'2 Comments',\n",
      "  'reaction': 17,\n",
      "  'share': '',\n",
      "  'url': 'https://m.facebook.com/avn.living.wisdom',\n",
      "  'user': 'fans of the avn shared australian national review'}]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import pprint as pp\n",
    "from collections import OrderedDict\n",
    "\n",
    "#input login credentials\n",
    "facebookusername = 'YOUR_USERNAME'\n",
    "facebookpassword = 'YOUR_PASSWORD'\n",
    "\n",
    "path_to_chromedriver = '/PATH/TO/chromedriver'            # change path as needed\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver)\n",
    "\n",
    "#url = raw_input(['Enter your facebook group or page URL: ']).replace('www', 'm') + '/'\n",
    "\n",
    "#sample pages to test against\n",
    "#url = 'https://m.facebook.com/vaccinetruth/'\n",
    "#url = 'https://m.facebook.com/stopavn/'            \n",
    "#url = 'https://m.facebook.com/vaccinetruth/'            \n",
    "url = 'https://m.facebook.com/avn.living.wisdom/'\n",
    "#url = 'https://m.facebook.com/RtAVM/'\n",
    "\n",
    "\n",
    "#function to handle browser login - using Selenium\n",
    "def fb_html(u):\n",
    "    \n",
    "    browser.get(u)\n",
    "    \n",
    "    try: \n",
    "        #fb mobile site login steps \n",
    "        browser.find_element_by_xpath('//*[@id=\"m_loginbar_login_button\"]').send_keys(Keys.RETURN)\n",
    "        browser.find_element_by_xpath('//*[@id=\"u_0_1\"]/div[1]/div/input').send_keys(facebookusername)\n",
    "        browser.find_element_by_xpath('//*[@id=\"u_0_1\"]/div[1]/div/input').send_keys(Keys.TAB, facebookpassword)\n",
    "        browser.find_element_by_xpath('//*[@id=\"u_0_1\"]/div[1]/div/input').send_keys(Keys.TAB, Keys.TAB, Keys.TAB, Keys.RETURN)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        #alernative facebook_m login\n",
    "        \n",
    "        try: \n",
    "            browser.find_element_by_xpath('//*[@id=\"login_form\"]/ul/li[1]/input').send_keys(facebookusername)\n",
    "            browser.find_element_by_xpath('//*[@id=\"login_form\"]/ul/li[2]/div/input').send_keys(facebookpassword)\n",
    "            browser.find_element_by_xpath('//*[@id=\"login_form\"]/ul/li[3]/input').send_keys(Keys.RETURN)\n",
    "           \n",
    "        except NoSuchElementException:\n",
    "            browser.find_element_by_xpath('//*[@id=\"u_0_1\"]/div[1]/div/input').send_keys(facebookusername)\n",
    "            browser.find_element_by_xpath('//*[@id=\"u_0_2\"]').send_keys(facebookpassword)\n",
    "            browser.find_element_by_xpath('//*[@id=\"u_0_6\"]').send_keys(Keys.RETURN)\n",
    "\n",
    "\n",
    "    #call fb page handling functions - collapsibles + dynamic page scrolling function    \n",
    "    fb_expander(browser)\n",
    "    \n",
    "    #source HTML for scraping\n",
    "    html = browser.page_source\n",
    "    \n",
    "    return html\n",
    "\n",
    "\n",
    "\n",
    "#function to handle dynamic page content loading - using Selenium\n",
    "def fb_scroller(browser):\n",
    "    \n",
    "    #define initial page height for 'while' loop\n",
    "    lastHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        #define how many seconds to wait while dynamic page content loads\n",
    "        time.sleep(3)\n",
    "        newHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if newHeight == lastHeight:\n",
    "            break\n",
    "        else:\n",
    "            lastHeight = newHeight\n",
    "    \n",
    "    return browser\n",
    "\n",
    "\n",
    "#function to handle collapsible section pages - expands the 2015 pages only\n",
    "def fb_expander(browser):\n",
    "    \n",
    "    #define initial page height for 'while' loop\n",
    "    lastHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        #click the '2015' section expander\n",
    "        browser.find_element_by_xpath('//*[@id=\"u_0_e\"]/div[2]/div[2]/a/div[1]/div/h3/div/div').click()\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        fb_scroller(browser)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(3)\n",
    "            #click the 'see more' expander for 2015 entries\n",
    "            browser.find_element_by_xpath('//*[starts-with(@id, \"u_\")]/div[1]/div/h1/div/div').click()\n",
    "        \n",
    "            #capture new page height\n",
    "            newHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "  \n",
    "            if lastHeight == newHeight:\n",
    "                #break\n",
    "                fb_scroller(browser)\n",
    "            else:\n",
    "                lastHeight = newHeight\n",
    "                \n",
    "    except:\n",
    "        fb_scroller(browser)\n",
    "    \n",
    "    return browser\n",
    "\n",
    "\n",
    "    \n",
    "#function to handle/parse HTML and extract data - using BeautifulSoup    \n",
    "def blogxtract(url):\n",
    "    \n",
    "    problemchars = re.compile(r'[\\[=\\+/&<>;:!\\\\|*^\\'\"\\?%#$@)(_\\,\\.\\t\\r\\n0-9-—\\]]')\n",
    "    prochar = '[(=\\-\\+\\:/&<>;|\\'\"\\?%#$@\\,\\._)]'\n",
    "    like = re.compile(r'(.*)(?= L)')\n",
    "    comment = re.compile(r'(.*)(?= C)')\n",
    "    share = re.compile(r'(?<=Comments )(.*)(?= S)')\n",
    "    \n",
    "    blog_list = []\n",
    "        \n",
    "    \n",
    "    soup = BeautifulSoup(fb_html(url), \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        for i in soup.find_all('div', {\"class\": \"_3w7e\"}):\n",
    "\n",
    "            text_list = []\n",
    "            text_list_final = []\n",
    "\n",
    "            #metadata builder\n",
    "            user = (i.find(re.compile('h1|h3')).text[0:50].lower().encode('ascii', 'ignore').strip() if i.find(re.compile('h1|h3')) is not None else \"\")\n",
    "            #link = (\"https://m.facebook.com\" + i.strong.a['href'] if i.strong is not None else \"\")\n",
    "            link = (\"https://m.facebook.com/\" + url.rsplit('/',2)[1])\n",
    "            date = (time.strftime(\"%d/%m/%Y\") if 'hr' in (i.find('abbr').get_text() if i.find('abbr') is not None else \"\") else (i.parent.find('abbr').get_text() if i.parent.find('abbr') is not None else \"\"))         \n",
    "            popular = (re.findall(r\"[^\\W\\d_]+|\\d+\", i.find('div', {\"class\": \"_1fnt\"}).get_text()))\n",
    "            popular_text = ' '.join(popular).replace('LikeCommentShare','')\n",
    "            react = (re.findall(r\"[^\\W\\d_]+|\\d+\", i.find('div', {\"class\": \"_1g06\"}).get_text()))\n",
    "\n",
    "            #blog text builder\n",
    "            for k in i.find_all('p'):\n",
    "                text_list.append(k.get_text().lower().replace('\\n',' ').replace(\"'\", \"\").encode('ascii', 'ignore').strip() if k is not None else \"\")\n",
    "\n",
    "\n",
    "            #replace bad characters in blog text\n",
    "            for ch in prochar:\n",
    "                for l in text_list:\n",
    "                    if ch in l:\n",
    "                        l = problemchars.sub(' ', l).strip()\n",
    "                        text_list_final.append(l)\n",
    "\n",
    "            #build dictionary\n",
    "            blog_dict = {\n",
    "            \"header\": \"facebook_group_\" + url.rsplit('/',2)[1],\n",
    "            \"url\": link,\n",
    "            \"user\": user,\n",
    "            \"date\": date,\n",
    "            \"popular\": popular_text,\n",
    "            \"blog_text\": ' '.join(list(OrderedDict.fromkeys(text_list_final))).replace('likes      likes   comments likes      likes likes',''),\n",
    "            \"comment\": (int(''.join((comment.findall(str(popular_text)))).replace(' ','')) if len(comment.findall(str(popular_text))) > 0 else ''),\n",
    "            \"share\": (int(''.join((share.findall(str(popular_text)))).replace(' ','')) if len(share.findall(str(popular_text))) > 0 else ''),\n",
    "            \"reaction\": (int(''.join(react)))\n",
    "                    }\n",
    "\n",
    "            blog_list.append(blog_dict)\n",
    "\n",
    "\n",
    "    #error handling  \n",
    "    except (AttributeError, TypeError, ValueError):\n",
    "        print \"missing_value\"\n",
    "        \n",
    "            \n",
    "    #call csv writer function and output file\n",
    "    writer_csv_3(blog_list)\n",
    "    \n",
    "    return pp.pprint(blog_list[0:4])\n",
    "\n",
    "    \n",
    "    \n",
    "#function to write CSV file\n",
    "def writer_csv_3(blog_list):\n",
    "    \n",
    "    #uses group name from URL to construct output file name\n",
    "    file_out = \"facebook_group_{page}.csv\".format(page = url.rsplit('/',2)[1])\n",
    "    \n",
    "    with open(file_out, 'w') as csvfile:\n",
    "\n",
    "        writer = csv.writer(csvfile, lineterminator='\\n', delimiter=',', quotechar='\"')\n",
    "        \n",
    "        writer.writerow([\"header\", \"url\", \"user\", \"date\", \"popualar\", \"blog_text\", \"comment\", \"share\", \"reaction\"])\n",
    "    \n",
    "        for i in blog_list:\n",
    "            if len(i['blog_text']) > 0:\n",
    "                newrow = i['header'], i['url'], i['user'], i['date'], i['popular'], i['blog_text'], i[\"comment\"], i[\"share\"], i[\"reaction\"]\n",
    "                writer.writerow(newrow)                     \n",
    "            #else:\n",
    "            #    pass\n",
    "    \n",
    "    \n",
    "#tip the domino\n",
    "if __name__ == \"__main__\":\n",
    "    blogxtract(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
